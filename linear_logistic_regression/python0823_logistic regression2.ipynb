{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce59901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons #머신 러닝 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0141c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc58cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #그림 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db1bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, Matrix, Function, simplify, exp, hessian, solve, init_printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97ab1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf57eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y, y_hat):\n",
    "    lostt = -np.mean(y*(np.log(y_hat))-(1-y)*np.log(1-y_hat))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0da002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X,y,y_hat):\n",
    "    #x--->입력\n",
    "    #y---> 정답(target/label)\n",
    "    #y_hat ---> 가설(모델의 출력, hypothesis/예측치(prediction/추정))\n",
    "    #w ---> weight(파라미터, theta / 우리가 구하고자 하는값)\n",
    "    #b ---> bias(파라미터 , theta 0)\n",
    "    #m ---> 학습(training ) 데이터의 갯수\n",
    "    \n",
    "    m=X.shape[0]\n",
    "    \n",
    "    #Cost(Loss)를 weight로 미분함\n",
    "    dw = (1/m)*np.dot(X.T,(y_hat-y))\n",
    "    #Cost(Loss)를 bias로 미분함\n",
    "    db = (1/m)*np.sum((y_hat-y))\n",
    "    \n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683deedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#미분의 정의함수\n",
    "def derivative(f,x):\n",
    "    h=0.00001\n",
    "    return (f(x+h)-f(x))/h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6faeedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X,w,b):\n",
    "    # X = 입력\n",
    "    # w = 가중치\n",
    "    # b = bais\n",
    "    \n",
    "    # 직선은 y = mx + c\n",
    "    #그래서 직선의 방정식 mx+c = w.X + b\n",
    "    # m과 c를 풀어라\n",
    "    x1=[min(x[:,0]), max(x[:0])]\n",
    "    m=-b/w[1]\n",
    "    x2=m*x1+c\n",
    "    \n",
    "    #그림 그리기\n",
    "    fig = plt.figure(figsize=(10,0))\n",
    "    plt.plot(x[:,0][y==0],X[:,1][y==0],\"g^\")\n",
    "    plt.plot(x[:,0][y==1],X[:,1][y==1],\"bs\")\n",
    "    plt.xlim([-2,2]) #x 구간 지정\n",
    "    plt.ylim([0,2,2]) #ylimit(y구간 지정) \n",
    "    plt.xlabel(\"feature 1\")\n",
    "    plt.ylabel(\"feature 2\")\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.plot(x1,x2,'y-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea1ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    # X <=입력\n",
    "    # m <= training 개수\n",
    "    # n <= feature의 갯수(weight와 내적하는 것)\n",
    "    m,n = X.shape #(m,n) m행 n열\n",
    "    #X 행렬의 모든 n개의 feature들을 정규화함\n",
    "    for i in range(n):\n",
    "        X = (X-X.mean(axis=0))/X.std(axis=0)\n",
    "        #어제 했던 수식(데이터와 평균의 차이를) 표준편차(standard deviation)으로 나눔\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d3e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 함수\n",
    "## 이 함수에서 gradient descent를 반복하여 학습을 하고 weight와 bais를 구함\n",
    "def train(X,y,bs,epochs,lr):\n",
    "    #X <= 입력\n",
    "    #y <= true/target\n",
    "    # epoch는 반복횟수\n",
    "    # lr = learning rate\n",
    "    # m <- 학습데이터의 수\n",
    "    # n <- feature의 수\n",
    "    \n",
    "    m,n = X.shape\n",
    "    #weight와 bias 초기화\n",
    "    w=np.zeros((n,1))\n",
    "    b=0\n",
    "    #y를 reshape함(형태를 맞춤)\n",
    "    y= y.reshape(m,1)\n",
    "    #입력데이터 normalize\n",
    "    x= normalize(x)\n",
    "    #loss를 저장하기 위한 빈 List 생성\n",
    "    losses=[]\n",
    "    #학습\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range((m-1)//bs+1):\n",
    "            #batch 정의 , SGD(Stocastric(통계적0) Gradient Descent )\n",
    "            start_i= i*bs #bs(배치 사이즈)\n",
    "            end_i = start_i+bs\n",
    "            xb = X[start_i:end_i]\n",
    "            yb = y[start_i:end_i]\n",
    "            \n",
    "            #hypothesis\n",
    "            y_hat = sigmoid(np.dot(xb,w)+b)\n",
    "            #loss를 파라미터로 미분\n",
    "            dw, db = gradient(xb,yb,y_hat)\n",
    "            #파라미터 갱신\n",
    "            w-=lr*dw\n",
    "            b-=lr*db\n",
    "        l= loss(y,sigmoid(np.dot(X,w)+b))\n",
    "        losses.append(i)\n",
    "    #weight와 bias, loss 반환\n",
    "    return w,b,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6add84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#예측함수\n",
    "def predict(X):\n",
    "    #X <- 입력\n",
    "    #입력 데이터 normalize\n",
    "    x = normalize(X)\n",
    "    \n",
    "    #예측 / 추정치 / y_hat 계산\n",
    "    preds = sigmoid(np.dot(X,w)+b)\n",
    "    #예측 데이터 저장 리스트 생성\n",
    "    pred_class=[]\n",
    "    #y_hat >= 0.5 -> 1로 결과출력\n",
    "    #y_hat < 0.5 -> 0 결과출력\n",
    "    pred_class=[1 if i > 0.5 else 0 for i in preds]\n",
    "    return np.array(pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8634cd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(-3,3,0.01)\n",
    "y=sigmoid(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c6314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "derivated = derivative(sigmoid,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44316eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097791fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X,derivated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2= symbols('x1 x2')\n",
    "f, g, h = symbols('f g h', cls=Function)\n",
    "\n",
    "X =Matrix([x1, x2])\n",
    "f = Matrix([-x1*x2*exp(-(x1**2 + x2**2)/2)])\n",
    "h=2*x1 +3*x2\n",
    "g=x1**2+x2**2-10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db435944",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradf = simplify(f.jacobian(X))\n",
    "gradf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53fba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "X,y = make_moons(ns)\n",
    "w,b,i = train(X,y,bs=100,epochs=1000,lr=0.01)\n",
    "#그림 그리기\n",
    "plot_decision_boundary(X,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4814a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b1eda6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da19c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
